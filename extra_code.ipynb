{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE EVALUATION OF ASL CLASSIFICATION RESULTS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "def evaluate_asl_predictions(results_df):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of ASL classification results\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with 'label' (true) and 'prediction' columns\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all evaluation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    y_true = results_df['label']\n",
    "    y_pred = results_df['prediction']\n",
    "    \n",
    "    # Remove any failed predictions (\"?\") for cleaner analysis\n",
    "    valid_mask = (y_pred != \"?\") & (y_true != \"?\") & (~pd.isna(y_pred)) & (~pd.isna(y_true))\n",
    "    y_true_clean = y_true[valid_mask]\n",
    "    y_pred_clean = y_pred[valid_mask]\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"🔍 ASL CLASSIFICATION EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. OVERALL ACCURACY\n",
    "    overall_accuracy = accuracy_score(y_true_clean, y_pred_clean)\n",
    "    failed_predictions = (~valid_mask).sum()\n",
    "    \n",
    "    print(f\"\\n📊 OVERALL METRICS:\")\n",
    "    print(f\"   Total Images: {len(results_df)}\")\n",
    "    print(f\"   Valid Predictions: {len(y_true_clean)} ({len(y_true_clean)/len(results_df)*100:.1f}%)\")\n",
    "    print(f\"   Failed Predictions: {failed_predictions} ({failed_predictions/len(results_df)*100:.1f}%)\")\n",
    "    print(f\"   Overall Accuracy: {overall_accuracy:.3f} ({overall_accuracy*100:.1f}%)\")\n",
    "    \n",
    "    # 2. PER-CLASS ACCURACY\n",
    "    print(f\"\\n📈 PER-LETTER ACCURACY:\")\n",
    "    per_class_accuracy = {}\n",
    "    letters_list = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "    \n",
    "    for letter in sorted(letters_list):\n",
    "        letter_mask = y_true_clean == letter\n",
    "        if letter_mask.sum() > 0:\n",
    "            letter_accuracy = (y_true_clean[letter_mask] == y_pred_clean[letter_mask]).mean()\n",
    "            per_class_accuracy[letter] = letter_accuracy\n",
    "            print(f\"   {letter}: {letter_accuracy:.3f} ({letter_accuracy*100:.1f}%) - {letter_mask.sum()} samples\")\n",
    "    \n",
    "    # 3. BEST AND WORST PERFORMING LETTERS\n",
    "    if per_class_accuracy:\n",
    "        sorted_accuracy = sorted(per_class_accuracy.items(), key=lambda x: x[1], reverse=True)\n",
    "        print(f\"\\n🏆 BEST PERFORMING LETTERS:\")\n",
    "        for letter, acc in sorted_accuracy[:5]:\n",
    "            print(f\"   {letter}: {acc:.3f} ({acc*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\n⚠️  WORST PERFORMING LETTERS:\")\n",
    "        for letter, acc in sorted_accuracy[-5:]:\n",
    "            print(f\"   {letter}: {acc:.3f} ({acc*100:.1f}%)\")\n",
    "    \n",
    "    # 4. CONFUSION ANALYSIS\n",
    "    print(f\"\\n🔄 MOST COMMON MISCLASSIFICATIONS:\")\n",
    "    confusion_pairs = []\n",
    "    for i in range(len(y_true_clean)):\n",
    "        if y_true_clean.iloc[i] != y_pred_clean.iloc[i]:\n",
    "            confusion_pairs.append((y_true_clean.iloc[i], y_pred_clean.iloc[i]))\n",
    "    \n",
    "    common_mistakes = Counter(confusion_pairs).most_common(10)\n",
    "    for (true_letter, pred_letter), count in common_mistakes:\n",
    "        print(f\"   {true_letter} → {pred_letter}: {count} times\")\n",
    "    \n",
    "    # 5. DETAILED CLASSIFICATION REPORT\n",
    "    print(f\"\\n📋 DETAILED CLASSIFICATION REPORT:\")\n",
    "    try:\n",
    "        report = classification_report(y_true_clean, y_pred_clean, zero_division=0)\n",
    "        print(report)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate classification report: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'per_class_accuracy': per_class_accuracy,\n",
    "        'failed_predictions': failed_predictions,\n",
    "        'total_samples': len(results_df),\n",
    "        'valid_samples': len(y_true_clean),\n",
    "        'y_true_clean': y_true_clean,\n",
    "        'y_pred_clean': y_pred_clean\n",
    "    }\n",
    "\n",
    "def plot_evaluation_results(results_df, save_plots=True):\n",
    "    \"\"\"\n",
    "    Create visualization plots for evaluation results\n",
    "    \"\"\"\n",
    "    y_true = results_df['label']\n",
    "    y_pred = results_df['prediction']\n",
    "    \n",
    "    # Remove failed predictions\n",
    "    valid_mask = (y_pred != \"?\") & (y_true != \"?\") & (~pd.isna(y_pred)) & (~pd.isna(y_true))\n",
    "    y_true_clean = y_true[valid_mask]\n",
    "    y_pred_clean = y_pred[valid_mask]\n",
    "    \n",
    "    if len(y_true_clean) == 0:\n",
    "        print(\"⚠️ No valid predictions to plot!\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('ASL Classification Evaluation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Confusion Matrix (simplified for readability)\n",
    "    letters_in_data = sorted(list(set(y_true_clean) | set(y_pred_clean)))\n",
    "    cm = confusion_matrix(y_true_clean, y_pred_clean, labels=letters_in_data)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=letters_in_data, yticklabels=letters_in_data, ax=axes[0,0])\n",
    "    axes[0,0].set_title('Confusion Matrix')\n",
    "    axes[0,0].set_xlabel('Predicted')\n",
    "    axes[0,0].set_ylabel('Actual')\n",
    "    \n",
    "    # 2. Per-class Accuracy\n",
    "    per_class_acc = {}\n",
    "    for letter in letters_in_data:\n",
    "        letter_mask = y_true_clean == letter\n",
    "        if letter_mask.sum() > 0:\n",
    "            per_class_acc[letter] = (y_true_clean[letter_mask] == y_pred_clean[letter_mask]).mean()\n",
    "    \n",
    "    if per_class_acc:\n",
    "        letters = list(per_class_acc.keys())\n",
    "        accuracies = list(per_class_acc.values())\n",
    "        \n",
    "        bars = axes[0,1].bar(letters, accuracies, color='skyblue', alpha=0.7)\n",
    "        axes[0,1].set_title('Per-Letter Accuracy')\n",
    "        axes[0,1].set_xlabel('Letters')\n",
    "        axes[0,1].set_ylabel('Accuracy')\n",
    "        axes[0,1].set_ylim(0, 1)\n",
    "        axes[0,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            if bar.get_height() > 0:\n",
    "                axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                              f'{acc:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 3. Sample Distribution\n",
    "    true_counts = y_true_clean.value_counts().sort_index()\n",
    "    axes[1,0].bar(true_counts.index, true_counts.values, color='lightgreen', alpha=0.7)\n",
    "    axes[1,0].set_title('Sample Distribution by Letter')\n",
    "    axes[1,0].set_xlabel('Letters')\n",
    "    axes[1,0].set_ylabel('Number of Samples')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Overall Statistics\n",
    "    overall_acc = accuracy_score(y_true_clean, y_pred_clean)\n",
    "    failed_preds = len(results_df) - len(y_true_clean)\n",
    "    \n",
    "    stats_text = f\"\"\"Overall Statistics:\n",
    "    \n",
    "Total Images: {len(results_df)}\n",
    "Valid Predictions: {len(y_true_clean)}\n",
    "Failed Predictions: {failed_preds}\n",
    "Overall Accuracy: {overall_acc:.3f}\n",
    "\n",
    "Letters Evaluated: {len(letters_in_data)}\n",
    "Unique True Labels: {len(set(y_true_clean))}\n",
    "Unique Predictions: {len(set(y_pred_clean))}\"\"\"\n",
    "    \n",
    "    axes[1,1].text(0.1, 0.5, stats_text, transform=axes[1,1].transAxes, \n",
    "                   fontsize=12, verticalalignment='center',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.5))\n",
    "    axes[1,1].set_xlim(0, 1)\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    axes[1,1].axis('off')\n",
    "    axes[1,1].set_title('Summary Statistics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_plots:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f'asl_evaluation_{timestamp}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"📊 Plots saved as: {filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def create_detailed_report(results_df, save_report=True):\n",
    "    \"\"\"\n",
    "    Create a detailed text report of the evaluation\n",
    "    \"\"\"\n",
    "    metrics = evaluate_asl_predictions(results_df)\n",
    "    \n",
    "    if save_report:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        report_filename = f'asl_evaluation_report_{timestamp}.txt'\n",
    "        \n",
    "        with open(report_filename, 'w') as f:\n",
    "            f.write(\"ASL CLASSIFICATION EVALUATION REPORT\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            \n",
    "            f.write(\"SUMMARY METRICS:\\n\")\n",
    "            f.write(f\"- Total Images: {metrics['total_samples']}\\n\")\n",
    "            f.write(f\"- Valid Predictions: {metrics['valid_samples']}\\n\")\n",
    "            f.write(f\"- Failed Predictions: {metrics['failed_predictions']}\\n\")\n",
    "            f.write(f\"- Overall Accuracy: {metrics['overall_accuracy']:.3f}\\n\\n\")\n",
    "            \n",
    "            if metrics['per_class_accuracy']:\n",
    "                f.write(\"PER-CLASS ACCURACY:\\n\")\n",
    "                for letter, acc in sorted(metrics['per_class_accuracy'].items()):\n",
    "                    f.write(f\"- {letter}: {acc:.3f}\\n\")\n",
    "        \n",
    "        print(f\"📄 Detailed report saved as: {report_filename}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# USAGE EXAMPLES:\n",
    "print(\"\"\"\n",
    "🚀 HOW TO USE THESE EVALUATION FUNCTIONS:\n",
    "\n",
    "1. After running your classification pipeline:\n",
    "   results_df = classification_pipeline(img_df)\n",
    "\n",
    "2. Get comprehensive evaluation:\n",
    "   metrics = evaluate_asl_predictions(results_df)\n",
    "\n",
    "3. Create visualizations:\n",
    "   plot_evaluation_results(results_df, save_plots=True)\n",
    "\n",
    "4. Generate detailed report:\n",
    "   create_detailed_report(results_df, save_report=True)\n",
    "\n",
    "5. Quick accuracy check:\n",
    "   accuracy = (results_df['label'] == results_df['prediction']).mean()\n",
    "   print(f'Quick Accuracy: {accuracy:.3f}')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40118fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    " \n",
    "ALPHABETS = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "\n",
    "def count_images_per_split(dataset_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Count images per ASL letter in train and test splits.\n",
    "\n",
    "    Args:\n",
    "        dataset_dir: Root dataset directory containing 'train' and 'test' subfolders.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: ['Letter', 'Train', 'Test', 'Total']\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_dir)\n",
    "    train_path = dataset_path / 'train'\n",
    "    test_path = dataset_path / 'test'\n",
    "\n",
    "\n",
    "    def count_in_split(split_path: Path) -> Dict[str, int]:\n",
    "        counts: Dict[str, int] = {letter: 0 for letter in ALPHABETS}\n",
    "        if not split_path.exists():\n",
    "            return counts\n",
    "        for letter in ALPHABETS:\n",
    "            letter_dir = split_path / letter\n",
    "            if letter_dir.exists() and letter_dir.is_dir():\n",
    "                num_files = sum(\n",
    "                    1 for p in letter_dir.iterdir()\n",
    "                    if p.is_file() and p.suffix.lower() in {'.jpg', '.jpeg', '.png'}\n",
    "                )\n",
    "                counts[letter] = num_files\n",
    "        return counts\n",
    "\n",
    "    train_counts = count_in_split(train_path)\n",
    "    test_counts = count_in_split(test_path)\n",
    "\n",
    "    rows = []\n",
    "    for letter in ALPHABETS:\n",
    "        tr = train_counts.get(letter, 0)\n",
    "        te = test_counts.get(letter, 0)\n",
    "        rows.append({\n",
    "            'Letter': letter,\n",
    "            'Train': tr,\n",
    "            'Test': te,\n",
    "            'Total': tr + te,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
