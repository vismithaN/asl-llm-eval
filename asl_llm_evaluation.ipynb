{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ASL Alphabet Recognition: Evaluating LLM Vision Capabilities for Accessibility\n",
        "\n",
        "## Research Overview\n",
        "This notebook evaluates current Large Language Models (LLMs) on their ability to identify American Sign Language (ASL) alphabets. The goal is to assess the current state of LLMs in the area of accessibility, specifically their vision capabilities for sign language recognition.\n",
        "\n",
        "### Methodology\n",
        "- **Task**: Supervised classification of ASL alphabet images (A-Z)\n",
        "- **Approach**: Feed individual ASL alphabet images to various LLMs and compare their predictions with ground truth labels\n",
        "- **Metrics**: Accuracy, confusion matrix, per-class performance, and response time\n",
        "- **Models to Evaluate**: GPT-4V, Claude 3, Gemini Pro Vision, and others\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/v202/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "Hey\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/v202/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# 1. Import Required Libraries\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import base64\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "from io import BytesIO\n",
        "\n",
        "# For ML metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# For API calls\n",
        "import openai\n",
        "import anthropic\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Hey\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Configuration and API Setup\n",
        "CONFIG = {\n",
        "    'data_path': './asl_alphabet_dataset',  # Path to your ASL dataset\n",
        "    'results_path': './evaluation_results',\n",
        "    'api_keys': {\n",
        "        'openai': os.getenv('OPENAI_API_KEY', ''),\n",
        "        'anthropic': os.getenv('ANTHROPIC_API_KEY', 'your-anthropic-key-here'),\n",
        "        'google': os.getenv('GOOGLE_API_KEY', 'your-google-key-here')\n",
        "    },\n",
        "    'models': {\n",
        "        'gpt4v': 'gpt-5',\n",
        "        'claude': 'claude-3-opus-20240229',\n",
        "        'gemini': 'gemini-pro-vision'\n",
        "    },\n",
        "    'evaluation': {\n",
        "        'max_samples_per_class': 10,  # Number of images per letter to evaluate\n",
        "        'timeout': 30,  # API timeout in seconds\n",
        "        'retry_attempts': 3\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create results directory\n",
        "Path(CONFIG['results_path']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ASL Alphabet classes (A-Z)\n",
        "ASL_CLASSES = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
        "print(f\"ASL Classes: {ASL_CLASSES}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Data Loading Functions\n",
        "class ASLDataLoader:\n",
        "    \"\"\"Load and manage ASL alphabet images for evaluation\"\"\"\n",
        "    \n",
        "    def __init__(self, data_path: str):\n",
        "        self.data_path = Path(data_path)\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        \n",
        "    def load_dataset(self, max_per_class: Optional[int] = None) -> Tuple[List, List]:\n",
        "        \"\"\"\n",
        "        Load ASL images from directory structure\n",
        "        \n",
        "        Args:\n",
        "            max_per_class: Maximum number of images to load per class\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of (image_paths, labels)\n",
        "        \"\"\"\n",
        "        if not self.data_path.exists():\n",
        "            print(f\"Warning: Dataset path {self.data_path} does not exist!\")\n",
        "            print(\"Creating sample dataset structure...\")\n",
        "            self._create_sample_structure()\n",
        "            return [], []\n",
        "        \n",
        "        for letter in ASL_CLASSES:\n",
        "            letter_path = self.data_path / letter\n",
        "            if letter_path.exists():\n",
        "                image_files = list(letter_path.glob('*.jpg')) + \\\n",
        "                             list(letter_path.glob('*.png')) + \\\n",
        "                             list(letter_path.glob('*.jpeg'))\n",
        "                \n",
        "                # Limit samples if specified\n",
        "                if max_per_class:\n",
        "                    image_files = image_files[:max_per_class]\n",
        "                \n",
        "                for img_file in image_files:\n",
        "                    self.images.append(str(img_file))\n",
        "                    self.labels.append(letter)\n",
        "        \n",
        "        print(f\"Loaded {len(self.images)} images across {len(set(self.labels))} classes\")\n",
        "        return self.images, self.labels\n",
        "    \n",
        "    def _create_sample_structure(self):\n",
        "        \"\"\"Create sample directory structure for demonstration\"\"\"\n",
        "        for letter in ASL_CLASSES[:5]:  # Create folders for first 5 letters\n",
        "            (self.data_path / letter).mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"Created sample structure at {self.data_path}\")\n",
        "        print(\"Please add ASL alphabet images to the respective folders\")\n",
        "    \n",
        "    def get_sample_distribution(self) -> pd.DataFrame:\n",
        "        \"\"\"Get distribution of samples per class\"\"\"\n",
        "        if not self.labels:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        distribution = pd.Series(self.labels).value_counts().sort_index()\n",
        "        return pd.DataFrame({\n",
        "            'Letter': distribution.index,\n",
        "            'Count': distribution.values\n",
        "        })\n",
        "    \n",
        "    def encode_image_base64(self, image_path: str) -> str:\n",
        "        \"\"\"Encode image to base64 for API calls\"\"\"\n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "    \n",
        "    def visualize_samples(self, n_samples: int = 5):\n",
        "        \"\"\"Visualize random samples from dataset\"\"\"\n",
        "        if not self.images:\n",
        "            print(\"No images loaded!\")\n",
        "            return\n",
        "        \n",
        "        sample_indices = np.random.choice(len(self.images), \n",
        "                                        min(n_samples, len(self.images)), \n",
        "                                        replace=False)\n",
        "        \n",
        "        fig, axes = plt.subplots(1, len(sample_indices), figsize=(15, 3))\n",
        "        if len(sample_indices) == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for idx, ax in zip(sample_indices, axes):\n",
        "            img = Image.open(self.images[idx])\n",
        "            ax.imshow(img)\n",
        "            ax.set_title(f\"Label: {self.labels[idx]}\")\n",
        "            ax.axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Initialize data loader\n",
        "data_loader = ASLDataLoader(CONFIG['data_path'])\n",
        "images, labels = data_loader.load_dataset(max_per_class=CONFIG['evaluation']['max_samples_per_class'])\n",
        "\n",
        "# Show sample distribution\n",
        "if images:\n",
        "    distribution = data_loader.get_sample_distribution()\n",
        "    print(\"\\nDataset Distribution:\")\n",
        "    print(distribution)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM API Interfaces\n",
        "\n",
        "These classes provide unified interfaces for different LLM providers to perform ASL alphabet classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. LLM API Interfaces\n",
        "class BaseLLMEvaluator:\n",
        "    \"\"\"Base class for LLM evaluators\"\"\"\n",
        "    \n",
        "    def __init__(self, api_key: str, model_name: str):\n",
        "        self.api_key = api_key\n",
        "        self.model_name = model_name\n",
        "        self.responses = []\n",
        "        self.response_times = []\n",
        "    \n",
        "    def create_prompt(self, prompt_style: str = 'standard') -> str:\n",
        "        \"\"\"Create prompt for ASL classification\"\"\"\n",
        "        prompts = {\n",
        "            'standard': \"\"\"Look at this image of an American Sign Language (ASL) hand sign. \n",
        "                          Identify which letter of the alphabet (A-Z) is being shown.\n",
        "                          Respond with ONLY the single letter, nothing else.\"\"\",\n",
        "            \n",
        "            'detailed': \"\"\"This image shows a hand gesture representing a letter in American Sign Language (ASL).\n",
        "                          Please analyze the hand position, finger configuration, and orientation.\n",
        "                          Identify which letter from A to Z is being signed.\n",
        "                          Respond with only the letter (e.g., 'A', 'B', 'C', etc.).\"\"\",\n",
        "            \n",
        "            'few_shot': \"\"\"You are an ASL alphabet classifier. Given an image of a hand sign, \n",
        "                          identify the letter being shown.\n",
        "                          Examples of ASL signs:\n",
        "                          - Closed fist with thumb to the side = 'A'\n",
        "                          - Open palm with fingers together = 'B'\n",
        "                          - Curved hand in C-shape = 'C'\n",
        "                          \n",
        "                          Now look at the provided image and respond with only the letter being signed.\"\"\",\n",
        "            \n",
        "            'chain_of_thought': \"\"\"Analyze this ASL hand sign step by step:\n",
        "                                   1. First, observe the overall hand position\n",
        "                                   2. Note the finger configuration\n",
        "                                   3. Check thumb position\n",
        "                                   4. Identify the letter being signed\n",
        "                                   \n",
        "                                   Final answer (letter only):\"\"\"\n",
        "        }\n",
        "        return prompts.get(prompt_style, prompts['standard'])\n",
        "    \n",
        "    def extract_letter_from_response(self, response: str) -> str:\n",
        "        \"\"\"Extract single letter from LLM response\"\"\"\n",
        "        # Clean the response\n",
        "        response = response.strip().upper()\n",
        "        \n",
        "        # Try to find a single letter\n",
        "        import re\n",
        "        letters = re.findall(r'[A-Z]', response)\n",
        "        \n",
        "        if letters:\n",
        "            # If response is just a single letter, return it\n",
        "            if len(response) == 1 and response in ASL_CLASSES:\n",
        "                return response\n",
        "            # Otherwise, return the first letter found\n",
        "            return letters[0]\n",
        "        \n",
        "        return \"?\"  # Unknown if no letter found\n",
        "    \n",
        "    def evaluate_image(self, image_path: str, prompt_style: str = 'standard') -> Dict:\n",
        "        \"\"\"Evaluate a single image - to be implemented by subclasses\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class OpenAIEvaluator(BaseLLMEvaluator):\n",
        "    \"\"\"OpenAI GPT-4V evaluator\"\"\"\n",
        "    \n",
        "    def __init__(self, api_key: str, model_name: str = \"gpt-4-vision-preview\"):\n",
        "        super().__init__(api_key, model_name)\n",
        "        self.client = openai.OpenAI(api_key=api_key)\n",
        "    \n",
        "    def evaluate_image(self, image_path: str, prompt_style: str = 'standard') -> Dict:\n",
        "        \"\"\"Evaluate ASL image using GPT-4V\"\"\"\n",
        "        try:\n",
        "            # Encode image\n",
        "            base64_image = data_loader.encode_image_base64(image_path)\n",
        "            \n",
        "            # Create message\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": self.create_prompt(prompt_style)},\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\n",
        "                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "            \n",
        "            # Time the API call\n",
        "            start_time = time.time()\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model_name,\n",
        "                messages=messages,\n",
        "                max_tokens=50,\n",
        "                temperature=0\n",
        "            )\n",
        "            response_time = time.time() - start_time\n",
        "            \n",
        "            # Extract prediction\n",
        "            raw_response = response.choices[0].message.content\n",
        "            predicted_letter = self.extract_letter_from_response(raw_response)\n",
        "            \n",
        "            return {\n",
        "                'predicted': predicted_letter,\n",
        "                'raw_response': raw_response,\n",
        "                'response_time': response_time,\n",
        "                'model': self.model_name\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'predicted': '?',\n",
        "                'raw_response': f\"Error: {str(e)}\",\n",
        "                'response_time': 0,\n",
        "                'model': self.model_name\n",
        "            }\n",
        "\n",
        "\n",
        "class AnthropicEvaluator(BaseLLMEvaluator):\n",
        "    \"\"\"Anthropic Claude evaluator\"\"\"\n",
        "    \n",
        "    def __init__(self, api_key: str, model_name: str = \"claude-3-opus-20240229\"):\n",
        "        super().__init__(api_key, model_name)\n",
        "        self.client = anthropic.Anthropic(api_key=api_key)\n",
        "    \n",
        "    def evaluate_image(self, image_path: str, prompt_style: str = 'standard') -> Dict:\n",
        "        \"\"\"Evaluate ASL image using Claude\"\"\"\n",
        "        try:\n",
        "            # Encode image\n",
        "            base64_image = data_loader.encode_image_base64(image_path)\n",
        "            \n",
        "            # Create message\n",
        "            start_time = time.time()\n",
        "            message = self.client.messages.create(\n",
        "                model=self.model_name,\n",
        "                max_tokens=50,\n",
        "                temperature=0,\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": [\n",
        "                            {\n",
        "                                \"type\": \"image\",\n",
        "                                \"source\": {\n",
        "                                    \"type\": \"base64\",\n",
        "                                    \"media_type\": \"image/jpeg\",\n",
        "                                    \"data\": base64_image\n",
        "                                }\n",
        "                            },\n",
        "                            {\n",
        "                                \"type\": \"text\",\n",
        "                                \"text\": self.create_prompt(prompt_style)\n",
        "                            }\n",
        "                        ]\n",
        "                    }\n",
        "                ]\n",
        "            )\n",
        "            response_time = time.time() - start_time\n",
        "            \n",
        "            # Extract prediction\n",
        "            raw_response = message.content[0].text\n",
        "            predicted_letter = self.extract_letter_from_response(raw_response)\n",
        "            \n",
        "            return {\n",
        "                'predicted': predicted_letter,\n",
        "                'raw_response': raw_response,\n",
        "                'response_time': response_time,\n",
        "                'model': self.model_name\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'predicted': '?',\n",
        "                'raw_response': f\"Error: {str(e)}\",\n",
        "                'response_time': 0,\n",
        "                'model': self.model_name\n",
        "            }\n",
        "\n",
        "\n",
        "class GeminiEvaluator(BaseLLMEvaluator):\n",
        "    \"\"\"Google Gemini Pro Vision evaluator\"\"\"\n",
        "    \n",
        "    def __init__(self, api_key: str, model_name: str = \"gemini-pro-vision\"):\n",
        "        super().__init__(api_key, model_name)\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "    \n",
        "    def evaluate_image(self, image_path: str, prompt_style: str = 'standard') -> Dict:\n",
        "        \"\"\"Evaluate ASL image using Gemini\"\"\"\n",
        "        try:\n",
        "            # Load image\n",
        "            img = Image.open(image_path)\n",
        "            \n",
        "            # Generate content\n",
        "            start_time = time.time()\n",
        "            response = self.model.generate_content([self.create_prompt(prompt_style), img])\n",
        "            response_time = time.time() - start_time\n",
        "            \n",
        "            # Extract prediction\n",
        "            raw_response = response.text\n",
        "            predicted_letter = self.extract_letter_from_response(raw_response)\n",
        "            \n",
        "            return {\n",
        "                'predicted': predicted_letter,\n",
        "                'raw_response': raw_response,\n",
        "                'response_time': response_time,\n",
        "                'model': self.model_name\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'predicted': '?',\n",
        "                'raw_response': f\"Error: {str(e)}\",\n",
        "                'response_time': 0,\n",
        "                'model': self.model_name\n",
        "            }\n",
        "\n",
        "\n",
        "# Initialize evaluators (only if API keys are set)\n",
        "evaluators = {}\n",
        "\n",
        "if CONFIG['api_keys']['openai'] != 'your-openai-key-here':\n",
        "    evaluators['GPT-4V'] = OpenAIEvaluator(\n",
        "        CONFIG['api_keys']['openai'],\n",
        "        CONFIG['models']['gpt4v']\n",
        "    )\n",
        "    print(\"✓ OpenAI GPT-4V evaluator initialized\")\n",
        "\n",
        "if CONFIG['api_keys']['anthropic'] != 'your-anthropic-key-here':\n",
        "    evaluators['Claude'] = AnthropicEvaluator(\n",
        "        CONFIG['api_keys']['anthropic'],\n",
        "        CONFIG['models']['claude']\n",
        "    )\n",
        "    print(\"✓ Anthropic Claude evaluator initialized\")\n",
        "\n",
        "if CONFIG['api_keys']['google'] != 'your-google-key-here':\n",
        "    evaluators['Gemini'] = GeminiEvaluator(\n",
        "        CONFIG['api_keys']['google'],\n",
        "        CONFIG['models']['gemini']\n",
        "    )\n",
        "    print(\"✓ Google Gemini evaluator initialized\")\n",
        "\n",
        "if not evaluators:\n",
        "    print(\"⚠️ No evaluators initialized. Please set API keys in the CONFIG section.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 5. Evaluation Pipeline\n",
        "class ASLEvaluationPipeline:\n",
        "    \"\"\"Main pipeline for evaluating LLMs on ASL alphabet classification\"\"\"\n",
        "    \n",
        "    def __init__(self, evaluators: Dict, data_loader: ASLDataLoader):\n",
        "        self.evaluators = evaluators\n",
        "        self.data_loader = data_loader\n",
        "        self.results = []\n",
        "        \n",
        "    def run_evaluation(self, \n",
        "                       images: List[str], \n",
        "                       labels: List[str],\n",
        "                       prompt_styles: List[str] = ['standard'],\n",
        "                       save_results: bool = True) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Run evaluation across all models and images\n",
        "        \n",
        "        Args:\n",
        "            images: List of image paths\n",
        "            labels: List of true labels\n",
        "            prompt_styles: List of prompt styles to test\n",
        "            save_results: Whether to save results to file\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with evaluation results\n",
        "        \"\"\"\n",
        "        \n",
        "        if not images:\n",
        "            print(\"No images to evaluate!\")\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        total_evaluations = len(images) * len(self.evaluators) * len(prompt_styles)\n",
        "        print(f\"Starting evaluation: {total_evaluations} total predictions\")\n",
        "        print(f\"Images: {len(images)}, Models: {len(self.evaluators)}, Prompt styles: {len(prompt_styles)}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        with tqdm(total=total_evaluations, desc=\"Evaluating\") as pbar:\n",
        "            for prompt_style in prompt_styles:\n",
        "                for img_path, true_label in zip(images, labels):\n",
        "                    for model_name, evaluator in self.evaluators.items():\n",
        "                        # Evaluate image\n",
        "                        result = evaluator.evaluate_image(img_path, prompt_style)\n",
        "                        \n",
        "                        # Store result\n",
        "                        self.results.append({\n",
        "                            'image_path': img_path,\n",
        "                            'true_label': true_label,\n",
        "                            'predicted_label': result['predicted'],\n",
        "                            'model': model_name,\n",
        "                            'prompt_style': prompt_style,\n",
        "                            'raw_response': result['raw_response'],\n",
        "                            'response_time': result['response_time'],\n",
        "                            'correct': result['predicted'] == true_label,\n",
        "                            'timestamp': datetime.now().isoformat()\n",
        "                        })\n",
        "                        \n",
        "                        pbar.update(1)\n",
        "                        \n",
        "                        # Small delay to avoid rate limiting\n",
        "                        time.sleep(0.1)\n",
        "        \n",
        "        # Convert to DataFrame\n",
        "        results_df = pd.DataFrame(self.results)\n",
        "        \n",
        "        # Save results\n",
        "        if save_results and len(results_df) > 0:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            results_path = Path(CONFIG['results_path']) / f\"asl_evaluation_{timestamp}.csv\"\n",
        "            results_df.to_csv(results_path, index=False)\n",
        "            print(f\"\\nResults saved to: {results_path}\")\n",
        "        \n",
        "        return results_df\n",
        "    \n",
        "    def run_single_image_test(self, image_path: str, true_label: str = None) -> pd.DataFrame:\n",
        "        \"\"\"Test all models on a single image\"\"\"\n",
        "        results = []\n",
        "        \n",
        "        print(f\"\\nTesting image: {image_path}\")\n",
        "        if true_label:\n",
        "            print(f\"True label: {true_label}\")\n",
        "        print(\"-\" * 30)\n",
        "        \n",
        "        for model_name, evaluator in self.evaluators.items():\n",
        "            result = evaluator.evaluate_image(image_path, 'standard')\n",
        "            print(f\"{model_name}: {result['predicted']} (Time: {result['response_time']:.2f}s)\")\n",
        "            \n",
        "            results.append({\n",
        "                'model': model_name,\n",
        "                'predicted': result['predicted'],\n",
        "                'response_time': result['response_time'],\n",
        "                'raw_response': result['raw_response']\n",
        "            })\n",
        "        \n",
        "        return pd.DataFrame(results)\n",
        "    \n",
        "    def compare_prompt_strategies(self, sample_images: List[str], sample_labels: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"Compare different prompting strategies on a sample of images\"\"\"\n",
        "        prompt_styles = ['standard', 'detailed', 'few_shot', 'chain_of_thought']\n",
        "        results = []\n",
        "        \n",
        "        print(\"\\nComparing prompt strategies...\")\n",
        "        print(f\"Testing {len(sample_images)} images with {len(prompt_styles)} prompt styles\")\n",
        "        \n",
        "        for img_path, true_label in zip(sample_images, sample_labels):\n",
        "            for prompt_style in prompt_styles:\n",
        "                for model_name, evaluator in self.evaluators.items():\n",
        "                    result = evaluator.evaluate_image(img_path, prompt_style)\n",
        "                    results.append({\n",
        "                        'true_label': true_label,\n",
        "                        'predicted_label': result['predicted'],\n",
        "                        'model': model_name,\n",
        "                        'prompt_style': prompt_style,\n",
        "                        'correct': result['predicted'] == true_label,\n",
        "                        'response_time': result['response_time']\n",
        "                    })\n",
        "        \n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "# Initialize evaluation pipeline\n",
        "pipeline = ASLEvaluationPipeline(evaluators, data_loader)\n",
        "\n",
        "print(\"Evaluation pipeline ready!\")\n",
        "print(f\"Available models: {list(evaluators.keys()) if evaluators else 'None'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Metrics and Visualization\n",
        "class MetricsAnalyzer:\n",
        "    \"\"\"Calculate and visualize evaluation metrics\"\"\"\n",
        "    \n",
        "    def __init__(self, results_df: pd.DataFrame):\n",
        "        self.results = results_df\n",
        "        \n",
        "    def calculate_overall_metrics(self) -> pd.DataFrame:\n",
        "        \"\"\"Calculate overall accuracy metrics per model\"\"\"\n",
        "        if self.results.empty:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        metrics = []\n",
        "        for model in self.results['model'].unique():\n",
        "            model_results = self.results[self.results['model'] == model]\n",
        "            \n",
        "            # Calculate metrics\n",
        "            accuracy = model_results['correct'].mean() * 100\n",
        "            avg_response_time = model_results['response_time'].mean()\n",
        "            total_predictions = len(model_results)\n",
        "            \n",
        "            metrics.append({\n",
        "                'Model': model,\n",
        "                'Accuracy (%)': round(accuracy, 2),\n",
        "                'Avg Response Time (s)': round(avg_response_time, 2),\n",
        "                'Total Predictions': total_predictions\n",
        "            })\n",
        "        \n",
        "        return pd.DataFrame(metrics).sort_values('Accuracy (%)', ascending=False)\n",
        "    \n",
        "    def calculate_per_class_accuracy(self, model_name: str) -> pd.DataFrame:\n",
        "        \"\"\"Calculate per-class accuracy for a specific model\"\"\"\n",
        "        model_results = self.results[self.results['model'] == model_name]\n",
        "        \n",
        "        if model_results.empty:\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        # Calculate accuracy per class\n",
        "        class_metrics = []\n",
        "        for letter in ASL_CLASSES:\n",
        "            class_results = model_results[model_results['true_label'] == letter]\n",
        "            if len(class_results) > 0:\n",
        "                accuracy = class_results['correct'].mean() * 100\n",
        "                class_metrics.append({\n",
        "                    'Letter': letter,\n",
        "                    'Accuracy (%)': round(accuracy, 2),\n",
        "                    'Samples': len(class_results)\n",
        "                })\n",
        "        \n",
        "        return pd.DataFrame(class_metrics)\n",
        "    \n",
        "    def plot_model_comparison(self):\n",
        "        \"\"\"Create bar plot comparing model accuracies\"\"\"\n",
        "        metrics = self.calculate_overall_metrics()\n",
        "        \n",
        "        if metrics.empty:\n",
        "            print(\"No data to plot!\")\n",
        "            return\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        bars = plt.bar(metrics['Model'], metrics['Accuracy (%)'])\n",
        "        \n",
        "        # Color code based on performance\n",
        "        colors = ['green' if acc >= 80 else 'orange' if acc >= 60 else 'red' \n",
        "                 for acc in metrics['Accuracy (%)']]\n",
        "        for bar, color in zip(bars, colors):\n",
        "            bar.set_color(color)\n",
        "        \n",
        "        plt.xlabel('Model', fontsize=12)\n",
        "        plt.ylabel('Accuracy (%)', fontsize=12)\n",
        "        plt.title('ASL Alphabet Classification Accuracy by Model', fontsize=14, fontweight='bold')\n",
        "        plt.ylim(0, 105)\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                    f'{height:.1f}%', ha='center', va='bottom')\n",
        "        \n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_confusion_matrix(self, model_name: str):\n",
        "        \"\"\"Plot confusion matrix for a specific model\"\"\"\n",
        "        model_results = self.results[self.results['model'] == model_name]\n",
        "        \n",
        "        if model_results.empty:\n",
        "            print(f\"No results for model: {model_name}\")\n",
        "            return\n",
        "        \n",
        "        # Get true and predicted labels\n",
        "        y_true = model_results['true_label'].values\n",
        "        y_pred = model_results['predicted_label'].values\n",
        "        \n",
        "        # Create confusion matrix\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=ASL_CLASSES)\n",
        "        \n",
        "        # Plot\n",
        "        plt.figure(figsize=(20, 18))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                   xticklabels=ASL_CLASSES, yticklabels=ASL_CLASSES,\n",
        "                   cbar_kws={'label': 'Count'})\n",
        "        plt.xlabel('Predicted Label', fontsize=12)\n",
        "        plt.ylabel('True Label', fontsize=12)\n",
        "        plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_per_class_performance(self):\n",
        "        \"\"\"Plot per-class accuracy for all models\"\"\"\n",
        "        fig, axes = plt.subplots(1, len(self.results['model'].unique()), \n",
        "                                figsize=(15, 6), sharey=True)\n",
        "        \n",
        "        if len(self.results['model'].unique()) == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for ax, model in zip(axes, self.results['model'].unique()):\n",
        "            class_metrics = self.calculate_per_class_accuracy(model)\n",
        "            \n",
        "            if not class_metrics.empty:\n",
        "                bars = ax.bar(class_metrics['Letter'], class_metrics['Accuracy (%)'])\n",
        "                \n",
        "                # Color code bars\n",
        "                for bar, acc in zip(bars, class_metrics['Accuracy (%)']):\n",
        "                    color = 'green' if acc >= 80 else 'orange' if acc >= 60 else 'red'\n",
        "                    bar.set_color(color)\n",
        "                \n",
        "                ax.set_xlabel('ASL Letter', fontsize=10)\n",
        "                ax.set_title(model, fontsize=12, fontweight='bold')\n",
        "                ax.set_ylim(0, 105)\n",
        "                ax.grid(axis='y', alpha=0.3)\n",
        "                \n",
        "                # Rotate x labels for better readability\n",
        "                ax.set_xticklabels(class_metrics['Letter'], rotation=45)\n",
        "        \n",
        "        axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "        plt.suptitle('Per-Class Accuracy Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_response_time_comparison(self):\n",
        "        \"\"\"Plot response time comparison across models\"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        # Create box plot\n",
        "        models = self.results['model'].unique()\n",
        "        response_times = [self.results[self.results['model'] == model]['response_time'].values \n",
        "                         for model in models]\n",
        "        \n",
        "        bp = plt.boxplot(response_times, labels=models, patch_artist=True)\n",
        "        \n",
        "        # Color the boxes\n",
        "        colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
        "        for patch, color in zip(bp['boxes'], colors[:len(models)]):\n",
        "            patch.set_facecolor(color)\n",
        "        \n",
        "        plt.xlabel('Model', fontsize=12)\n",
        "        plt.ylabel('Response Time (seconds)', fontsize=12)\n",
        "        plt.title('Response Time Distribution by Model', fontsize=14, fontweight='bold')\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_prompt_strategy_comparison(self):\n",
        "        \"\"\"Compare performance across different prompt strategies\"\"\"\n",
        "        if 'prompt_style' not in self.results.columns:\n",
        "            print(\"No prompt strategy comparison data available\")\n",
        "            return\n",
        "        \n",
        "        # Calculate accuracy for each model-prompt combination\n",
        "        pivot_data = self.results.pivot_table(\n",
        "            values='correct',\n",
        "            index='prompt_style',\n",
        "            columns='model',\n",
        "            aggfunc='mean'\n",
        "        ) * 100\n",
        "        \n",
        "        # Plot\n",
        "        ax = pivot_data.plot(kind='bar', figsize=(12, 6), rot=45)\n",
        "        plt.xlabel('Prompt Strategy', fontsize=12)\n",
        "        plt.ylabel('Accuracy (%)', fontsize=12)\n",
        "        plt.title('Performance by Prompt Strategy', fontsize=14, fontweight='bold')\n",
        "        plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def generate_classification_report(self, model_name: str):\n",
        "        \"\"\"Generate detailed classification report for a model\"\"\"\n",
        "        model_results = self.results[self.results['model'] == model_name]\n",
        "        \n",
        "        if model_results.empty:\n",
        "            print(f\"No results for model: {model_name}\")\n",
        "            return\n",
        "        \n",
        "        # Get classification report\n",
        "        report = classification_report(\n",
        "            model_results['true_label'],\n",
        "            model_results['predicted_label'],\n",
        "            labels=ASL_CLASSES,\n",
        "            zero_division=0\n",
        "        )\n",
        "        \n",
        "        print(f\"\\nClassification Report - {model_name}\")\n",
        "        print(\"=\" * 50)\n",
        "        print(report)\n",
        "        \n",
        "        return report\n",
        "    \n",
        "    def identify_common_mistakes(self, model_name: str, top_n: int = 10):\n",
        "        \"\"\"Identify most common misclassifications for a model\"\"\"\n",
        "        model_results = self.results[\n",
        "            (self.results['model'] == model_name) & \n",
        "            (self.results['correct'] == False)\n",
        "        ]\n",
        "        \n",
        "        if model_results.empty:\n",
        "            print(f\"No mistakes found for {model_name}!\")\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        # Create mistake pairs\n",
        "        mistakes = model_results.groupby(['true_label', 'predicted_label']).size().reset_index(name='count')\n",
        "        mistakes = mistakes.sort_values('count', ascending=False).head(top_n)\n",
        "        mistakes['mistake_pair'] = mistakes['true_label'] + ' → ' + mistakes['predicted_label']\n",
        "        \n",
        "        print(f\"\\nTop {top_n} Common Mistakes - {model_name}\")\n",
        "        print(\"=\" * 40)\n",
        "        for _, row in mistakes.iterrows():\n",
        "            print(f\"{row['mistake_pair']}: {row['count']} times\")\n",
        "        \n",
        "        return mistakes\n",
        "\n",
        "# Example usage (will only work if evaluation has been run)\n",
        "if 'results_df' in locals() and not results_df.empty:\n",
        "    analyzer = MetricsAnalyzer(results_df)\n",
        "    \n",
        "    # Show overall metrics\n",
        "    print(\"\\nOverall Model Performance:\")\n",
        "    print(analyzer.calculate_overall_metrics())\n",
        "    \n",
        "    # Create visualizations\n",
        "    analyzer.plot_model_comparison()\n",
        "else:\n",
        "    print(\"Run evaluation first to generate metrics and visualizations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "### Key Insights from Evaluation\n",
        "After running the evaluation, you should have insights on:\n",
        "1. **Overall Accuracy**: Which LLM performs best on ASL alphabet recognition\n",
        "2. **Per-Class Performance**: Which letters are easiest/hardest for LLMs to identify\n",
        "3. **Common Confusions**: Which letter pairs are frequently misclassified\n",
        "4. **Response Times**: Speed vs accuracy trade-offs\n",
        "5. **Prompt Engineering**: Which prompting strategies work best\n",
        "\n",
        "### Research Findings\n",
        "Document your findings regarding:\n",
        "- Current state of LLM vision capabilities for accessibility\n",
        "- Specific challenges in ASL recognition\n",
        "- Recommendations for improving LLM performance on sign language\n",
        "\n",
        "### Next Steps\n",
        "1. **Expand Dataset**: Test with more diverse ASL images (different backgrounds, hand positions, lighting)\n",
        "2. **Test More Models**: Include additional LLMs as they become available\n",
        "3. **Fine-tuning**: Consider fine-tuning models specifically for ASL recognition\n",
        "4. **Real-world Testing**: Test with real ASL users and varying hand appearances\n",
        "5. **Extended Alphabet**: Include numbers, common words, and phrases beyond just letters\n",
        "\n",
        "### Export Results\n",
        "Results are automatically saved to CSV files in the `evaluation_results` folder for further analysis.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
